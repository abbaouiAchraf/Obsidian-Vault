## Preface

The truth: machines don’t learn. What a typical “learning machine” does, is finding a mathematical formula, which, when applied to a collection of inputs (called “training data”), produces the desired outputs. This mathematical formula also generates the correct outputs for most other inputs (distinct from the training data) on the condition that those inputs come from the same or a similar statistical distribution as the one the training data was drawn from.

## Introduction

### What is Machine Learning

Machine learning is a subfield of computer science that is concerned with building algorithms which, to be useful, rely on a collection of examples of some phenomenon. These examples can come from nature, be handcrafted by humans or generated by another algorithm. 

Machine learning can also be defined as the process of solving a practical problem by 1) gathering a dataset, and 2) algorithmically building a statistical model based on that dataset. That statistical model is assumed to be used somehow to solve the practical problem. 

To save keystrokes, I use the terms “learning” and “machine learning” interchangeably.

### Types of Learning

Learning can be supervised, semi-supervised, unsupervised and reinforcement.

#### Supervised Learning
#### Unsupervised Learning
#### Semi-Supervised Learning

In semi-supervised learning, the dataset contains both labeled and unlabeled examples. Usually, the quantity of unlabeled examples is much higher than the number of labeled examples. The goal of a semi-supervised learning algorithm is the same as the goal of the supervised learning algorithm. The hope here is that using many unlabeled examples can help the learning algorithm to find (we might say “produce” or “compute”) a better model.

#### Reinforcement Learning

Reinforcement learning is a subfield of machine learning where the machine “lives” in an environment and is capable of perceiving the state of that environment as a vector of features. The machine can execute actions in every state. Diffrent actions bring diffrent rewards and could also move the machine to another state of the environment. The goal of a reinforcement learning algorithm is to learn a policy. A policy is a function f (similar to the model in supervised learning) that takes the feature vector of a state as input and outputs an optimal action to execute in that state. The action is optimal if it maximizes the expected average reward.



### Special note :

Converting *text* into a *feature vector*, called *bag of words*, is to take a dictionary of English words (let’s say it contains 20,000 alphabetically sorted words) and stipulate that in our feature vector: 
1. the first feature is equal to 1 if the email message contains the word “a”; otherwise, this feature is 0; 
2. the second feature is equal to 1 if the email message contains the word “aaron”; otherwise, this feature equals 0; 
3. ... 
4. the feature at position 20,000 is equal to 1 if the email message contains the word “zulu”; otherwise, this feature is equal to 0.
You repeat the above procedure for every email message in our collection, which gives us 10,000 feature vectors (each vector having the dimensionality of 20,000) and a label (“spam”/“not_spam”).

#### SVM (Support Vector Machine) :

This algorithm requires that the positive label (in our case it’s “spam”) has the numeric value of +1 (one), and the negative label (“not_spam”) has the value of -1 (minus one). 
At this point, you have a dataset and a learning algorithm, so you are ready to apply the learning algorithm to the dataset to get the model. 
SVM sees every feature vector as a point in a high-dimensional space (in our case, space is 20,000-dimensional). 
The algorithm puts all feature vectors on an imaginary 20,000- dimensional plot and draws an imaginary 20,000-dimensional line (a hyperplane) that separates examples with positive labels from examples with negative labels. In machine learning, the boundary separating the examples of different classes is called the *decision boundary*.

**The process of building a *Model* is called TRAINING**

When the vector is on the left side of the matrix in the multiplication, then it has to be transposed before we multiply it by the matrix. The transpose of the vector x denoted as x€ makes a row vector out of a column vector *x^(T).W*.

![[Pasted image 20230121192705.png]]

![[Pasted image 20230121200628.png]]

![[Pasted image 20230121203906.png]]

#### Model-Based vs. Instance-Based Learning

Most supervised learning algorithms are model-based. We have already seen one such algorithm: SVM. Model-based learning algorithms use the training data to create a model that has parameters learned from the training data. In SVM, the two parameters we saw were *w'* and *b'* . After the model was built, the training data can be discarded.
Instance-based learning algorithms use the whole dataset as the model. One instance-based algorithm frequently used in practice is **k-Nearest Neighbors (kNN)**. In classification, to predict a label for an input example the kNN algorithm looks at the close neighborhood of the input example in the space of feature vectors and outputs the label that it saw the most often in this close neighborhood.

#### Shallow vs. Deep Learning

A shallow learning algorithm learns the parameters of the model directly from the features of the training examples. Most supervised learning algorithms are shallow. The notorious exceptions are neural network learning algorithms, specifically those that build **neural networks** with more than one layer between input and output. Such neural networks are called **deep neural networks**. In deep neural network learning (or, simply, deep learning), contrary to shallow learning, most model parameters are learned not directly from the features of the training examples, but from the outputs of the preceding layers


### Fundamental Algorithms

#### 3.1 Linear Regression

Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example.
![[Pasted image 20230122142130.png]]
**Jusitification of using the linear form for the model**
![[Pasted image 20230122142520.png]]

#### 3.2 Logistic Regression

The first thing to say is that logistic regression is not a regression, but a classification learning algorithm. The name comes from statistics and is due to the fact that the mathematical formulation of logistic regression is similar to that of linear regression. 
I explain logistic regression on the case of binary classification. However, it can naturally be extended to multiclass classification.
![[Pasted image 20230122144015.png]]
![[Pasted image 20230124114831.png]]
![[Pasted image 20230124114916.png]]

#### 3.3 Decision Tree Learning
A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a specific feature j of the feature vector is examined. If the value of the feature is below a specific threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs.
![[Pasted image 20230124232511.png]]

#### 3.4 Support Vector Machine
##### Dealing with Inherent Non-Linearity
![[Pasted image 20230201153136.png]]
![[Pasted image 20230201153202.png]]
![[Pasted image 20230201153227.png]]

#### 3.4 k-Nearest Neighbors
k-Nearest Neighbors (kNN) is a non-parametric learning algorithm. Contrary to other learning algorithms that allow discarding the training data after the model is built, kNN keeps all training examples in memory. Once a new, previously unseen example x comes in, the kNN algorithm finds k training examples closest to x and returns the majority label (in case of classification) or the average label (in case of regression).

### Anatomy of a Learning Algorithm

**Gradient Descent or Stochastic Grandient Descent** : These are two most frequently used optimization algorithms used in cases where the optimization criterion is differentiable.

Gradient descent is an iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one starts at some random point and takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.
It can be used to find optimal parameters for linear and logistic regression, SVM and also neural networks which we consider later. For many models, such as logistic regression or SVM, the optimization criterion is convex. Convex functions have only one minimum, which is global. Optimization criteria for neural networks are not convex, but in practice even finding a local minimum suffices.

